{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Model training for the purpose of Research!<br>\n",
        "By Aashik Baruwal"
      ],
      "metadata": {
        "id": "aQWWoTet7cP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4rmZdtTZMCj0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your generate_questions_from_map.py script:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"Upload your concept map JSON files (upload as many as you need):\")\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "P3i3voT9Mrb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create necessary directories\n",
        "os.makedirs('scripts/data/concept_maps', exist_ok=True)\n",
        "os.makedirs('scripts/data/generated_questions', exist_ok=True)\n",
        "os.makedirs('scripts', exist_ok=True)\n",
        "\n",
        "# Move uploaded files to the correct locations\n",
        "for fname in os.listdir():\n",
        "    if fname.endswith('.json'):\n",
        "        shutil.move(fname, f'scripts/data/concept_maps/{fname}')\n",
        "    elif fname == 'generate_questions_from_map.py':\n",
        "        shutil.move(fname, f'scripts/{fname}')"
      ],
      "metadata": {
        "id": "4QbFPNe3MtNE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "!python scripts/generate_questions_from_map.py\n"
      ],
      "metadata": {
        "id": "t4j3iie_MuvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets -q\n",
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# You can replace this with your actual path\n",
        "cs_csv_path = '/content/scripts/data/generated_questions/cs_concept_map_questions.csv'\n",
        "sc_csv_path = '/content/scripts/data/generated_questions/sc_concept_map_questions.csv'\n",
        "\n",
        "# Loading the CSVs\n",
        "try:\n",
        "    cs_df = pd.read_csv(cs_csv_path)\n",
        "    sc_df = pd.read_csv(sc_csv_path)\n",
        "    print(\"‚úÖ Successfully loaded CSV files.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Make sure your CSV files are uploaded or Drive is mounted and paths are correct.\")\n",
        "    print(f\"Expected paths: {cs_csv_path}, {sc_csv_path}\")\n",
        "    # We will want to stop execution here if files aren't found\n",
        "\n",
        "\n",
        "# We will Combine both datasets\n",
        "if 'cs_df' in locals() and 'sc_df' in locals():\n",
        "    df = pd.concat([cs_df, sc_df], ignore_index=True)\n",
        "    print(f\"Combined dataframe shape: {df.shape}\")\n",
        "\n",
        "    # Prepare samples for QA training\n",
        "    samples = []\n",
        "    for index, row in df.iterrows():\n",
        "        # Simple context construction: Source Verb Target.\n",
        "        # We could potentially make this more complex or use the full sentence from source text if available\n",
        "        context = f\"{row['Source']} {row['Verb']} {row['Target']}.\"\n",
        "        question = row['Question']\n",
        "        answer = str(row['Answer']).strip() # Ensure answer is string and clean whitespace\n",
        "\n",
        "        # Find answer start index in the constructed context\n",
        "        # Use case-insensitive search for robustness\n",
        "        context_lower = context.lower()\n",
        "        answer_lower = answer.lower()\n",
        "        answer_start = context_lower.find(answer_lower)\n",
        "\n",
        "        # We need the answer to be present in the context for extractive QA\n",
        "        if answer_start != -1:\n",
        "             samples.append({\n",
        "                'context': context,\n",
        "                'question': question,\n",
        "                'answers': {\n",
        "                    'text': [answer],\n",
        "                    'answer_start': [answer_start]\n",
        "                }\n",
        "            })\n",
        "        # else:\n",
        "            # Optional: print skipped samples to debug\n",
        "            # print(f\"Skipped sample: Answer '{answer}' not found in context '{context}'\")\n",
        "\n",
        "\n",
        "    print(f\"Prepared {len(samples)} samples for training.\")\n",
        "\n",
        "    # Convert the training samples into a Huggingface Dataset\n",
        "    if samples:\n",
        "        dataset = Dataset.from_list(samples)\n",
        "        print(\"‚úÖ Converted samples to Huggingface Dataset.\")\n",
        "        print(dataset)\n",
        "    else:\n",
        "        print(\"‚ùå No valid training samples were prepared. Please check your CSV data and the data preparation logic.\")\n",
        "        dataset = None # Ensure dataset is None if no samples\n",
        "else:\n",
        "    print(\"‚ùå DataFrames were not loaded. Cannot proceed with data preparation.\")\n",
        "    dataset = None # Ensure dataset is None if no dataframes\n",
        "\n",
        "# Our selected model for low-end resource settings\n",
        "model_name = \"distilbert-base-uncased-distilled-squad\"\n",
        "\n",
        "\n",
        "if dataset is not None:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    print(f\"‚úÖ Loaded tokenizer and model: {model_name}\")\n",
        "\n",
        "    # Define a function to preprocess the dataset for tokenization\n",
        "    def preprocess_function(examples):\n",
        "        questions = examples[\"question\"]\n",
        "        contexts = examples[\"context\"]\n",
        "        answers = examples[\"answers\"]\n",
        "\n",
        "        # Tokenize the questions and contexts\n",
        "        inputs = tokenizer(\n",
        "            questions,\n",
        "            contexts,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=128, # Keep max length reasonable for low resource\n",
        "            return_offsets_mapping=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Extract offset mappings for token positions\n",
        "        offset_mappings = inputs.pop(\"offset_mapping\")\n",
        "\n",
        "        # Initialize lists to store start and end positions of answers\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        # Loop through each sample to calculate start and end token indices\n",
        "        for i, offset in enumerate(offset_mappings):\n",
        "            answer = answers[i]\n",
        "            start_char = answer[\"answer_start\"][0]\n",
        "            # Adding the length of the answer text to its start character index to get the end character index\n",
        "            end_char = start_char + len(answer[\"text\"][0])\n",
        "\n",
        "            # Find the sequence index of the context, typically 1 in [CLS] question [SEP] context [SEP]\n",
        "            sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "            # Adjust start and end char positions to be relative to the context\n",
        "            # Find the token corresponding to the start of the context\n",
        "            context_start_token = sequence_ids.index(1) if 1 in sequence_ids else 0 # Handle cases where context might not be explicitly separated\n",
        "\n",
        "            # Adjust start and end positions to find the answer within the context part of the tokenized sequence\n",
        "            # Find the token span of the answer\n",
        "            token_start_index = -1\n",
        "            token_end_index = -1\n",
        "\n",
        "            # Iterate through the tokens in the context part of the sequence\n",
        "            for token_index in range(context_start_token, len(sequence_ids)):\n",
        "                 if sequence_ids[token_index] != 1: # Stop when we leave the context part\n",
        "                     break\n",
        "                 # Check if the character range of the current token overlaps with the answer character range\n",
        "                 token_char_start = offset[token_index][0]\n",
        "                 token_char_end = offset[token_index][1]\n",
        "\n",
        "                 if token_start_index == -1 and token_char_start <= start_char and token_char_end >= start_char:\n",
        "                     token_start_index = token_index\n",
        "\n",
        "                 if token_char_start <= end_char and token_char_end >= end_char:\n",
        "                     token_end_index = token_index\n",
        "\n",
        "            # If token start/end found within context, use them\n",
        "            if token_start_index != -1 and token_end_index != -1:\n",
        "                 start_positions.append(token_start_index)\n",
        "                 end_positions.append(token_end_index)\n",
        "            else:\n",
        "                 # If answer tokens not found in context span, set positions to model's [CLS] token\n",
        "                 # This signifies an unanswerable question in SQuAD context\n",
        "                 start_positions.append(0)\n",
        "                 end_positions.append(0)\n",
        "\n",
        "\n",
        "        # Add start and end positions to the inputs\n",
        "        inputs[\"start_positions\"] = torch.tensor(start_positions)\n",
        "        inputs[\"end_positions\"] = torch.tensor(end_positions)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    # Apply the preprocessing function to the dataset\n",
        "    # This tokenizes the dataset and calculates start/end positions\n",
        "    # Ensure dataset is not None before mapping\n",
        "    if dataset is not None:\n",
        "        tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "        print(\"‚úÖ Successfully tokenized and preprocessed dataset.\")\n",
        "        print(tokenized_dataset)\n",
        "    else:\n",
        "         print(\"‚ùå Dataset is None. Skipping tokenization.\")\n",
        "         tokenized_dataset = None\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Dataset is None. Skipping model loading and tokenization.\")\n",
        "    tokenizer = None\n",
        "    model = None\n",
        "    tokenized_dataset = None\n",
        "\n",
        "\n",
        "if model is not None and tokenized_dataset is not None:\n",
        "    # Define training arguments for the model\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./qa_model\",  # Directory to save the model checkpoints\n",
        "        per_device_train_batch_size=8,  # Batch size per device - adjust based on Colab GPU RAM\n",
        "        num_train_epochs=2,  # Number of training epochs - start small\n",
        "        save_steps=200,  # Save the model every 200 steps - adjust based on dataset size\n",
        "        save_total_limit=1,  # Keep only the latest checkpoint\n",
        "        logging_steps=100,  # Log training progress every 100 steps\n",
        "        learning_rate=3e-5,  # Learning rate\n",
        "        weight_decay=0.01,  # Weight decay\n",
        "        disable_tqdm=False,  # Enable progress bars\n",
        "        push_to_hub=False,  # Do not push the model to Huggingface Hub\n",
        "        # Optional: Add evaluation if you have a separate eval dataset\n",
        "        # evaluation_strategy=\"steps\",\n",
        "        # eval_steps=200,\n",
        "        # load_best_model_at_end=True, # Requires evaluation strategy\n",
        "    )\n",
        "\n",
        "    # Initialize the Trainer for model training\n",
        "    trainer = Trainer(\n",
        "        model=model,  # The QA model\n",
        "        args=training_args,  # Training arguments\n",
        "        train_dataset=tokenized_dataset,  # Tokenized training dataset\n",
        "        tokenizer=tokenizer,  # Tokenizer for preprocessing\n",
        "    )\n",
        "\n",
        "    # Train the model using the Trainer\n",
        "    print(\"üöÄ Starting model training...\")\n",
        "    trainer.train()\n",
        "    print(\"‚úÖ Model training complete.\")\n",
        "    final_model_dir = \"./qa_model_final\"\n",
        "    trainer.save_model(final_model_dir)  # Save the model\n",
        "    tokenizer.save_pretrained(final_model_dir)  # Save the tokenizer\n",
        "\n",
        "    print(f\"‚úÖ Final model and tokenizer saved at {final_model_dir}\")\n",
        "else:\n",
        "    print(\"‚ùå Cannot proceed with training because dataset or model was not loaded.\")\n"
      ],
      "metadata": {
        "id": "iQkreoxzs0n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the saved model directory\n",
        "model_dir_to_zip = \"./qa_model_final\"\n",
        "zip_filename = \"qa_model_final.zip\"\n",
        "\n",
        "# Use a shell command to create the zip file\n",
        "!zip -r {zip_filename} {model_dir_to_zip}\n",
        "\n",
        "print(f\"‚úÖ Zipped model to {zip_filename}\")\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "\n",
        "zip_filename = \"qa_model_final.zip\" # Ensure this matches the filename used in the zipping step\n",
        "\n",
        "try:\n",
        "    files.download(zip_filename)\n",
        "    print(f\"‚úÖ Initiated download of {zip_filename}. Check your browser's downloads.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: {zip_filename} not found. Make sure the zipping step completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3jGAoi0-u2ww",
        "outputId": "cea30bdc-7d2f-4f03-e927-dfe650a723c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: qa_model_final/ (stored 0%)\n",
            "  adding: qa_model_final/special_tokens_map.json (deflated 42%)\n",
            "  adding: qa_model_final/tokenizer_config.json (deflated 75%)\n",
            "  adding: qa_model_final/model.safetensors (deflated 8%)\n",
            "  adding: qa_model_final/training_args.bin (deflated 52%)\n",
            "  adding: qa_model_final/config.json (deflated 43%)\n",
            "  adding: qa_model_final/tokenizer.json (deflated 71%)\n",
            "  adding: qa_model_final/vocab.txt (deflated 53%)\n",
            "‚úÖ Zipped model to qa_model_final.zip\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2ffdcb58-89db-4ed6-bf48-fba2eb53b91d\", \"qa_model_final.zip\", 244864656)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Initiated download of qa_model_final.zip. Check your browser's downloads.\n"
          ]
        }
      ]
    }
  ]
}